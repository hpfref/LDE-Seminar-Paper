\documentclass[sigconf]{acmart}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%dummy copyright and reference data
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}

\begin{document}


\title{Summary Paper of \\ \textit{Don’t Hold My Data Hostage – \\A Case For Client Protocol Redesign}}
\subtitle{Authors of Original Paper: Mark Raasveldt and Hannes Mühleisen}


\author{Hannes Pohnke}
%\authornote{Authors of summarized Paper: Mark Raasveldt and Hannes Mühleisen.}
\email{pohnke@tu-berlin.de}
\affiliation{
  \institution{TU Berlin}
  \city{Berlin}
  \country{Germany}
}


\begin{abstract}
Traditionally, database query processing and ML tasks are executed on separate, dedicated systems, but the current trend goes towards integrated data analysis pipelines that combine both tasks. In state of the art systems, orchestration of those two tasks still is inefficient due to expensive data transfer and missed global optimization potential. The paper we are summarizing, "Don’t Hold My Data Hostage – A Case For Client Protocol Redesign" (DHMDH) by Mark Raasveldt and Hannes Muhleisen, addresses this problem by investigating the high cost of transferring large data from databases to the client programs, which can be much more time consuming than the actual query execution. The authors explore and analyse current serialization methods, that are used in database systems and identify their inefficiencies through various experiments. They also introduce a new columnar serialization method that can significantly enhance data transfer performance. By improving the data transfer, this approach could be a step towards efficiently combining database and machine learning systems.
\end{abstract}

\keywords{Databases, Client Protocols, Data Export}

\maketitle

\section{Introduction}
Having to transfer a lot of data from a database to a client program is a standard procedure when conducting complex machine learning (ML) tasks. The problem is that this transfer process is very slow, especially because database servers are typically not located on the same system as the ML workloads. 

When the DHMDH paper was introduced in 2017 most research in this field, including the authors \cite{raasveldt2016vectorized}, focused on performing computations within the database, avoiding the need for data transfer. Since then a lot of approaches for optimizing the transfer arose, which can be broadly categorized in server-side and client-side optimizations \cite{neueres paper}. Raasveldt and Mühlheisen chose a client-side approach, as they already realized in 2017 that the biggest and easiest to implement optimization potential lies in addressing the overhead of (de)serialization of the data. 
They use the term of result set serialization (RSS), which refers to converting data into a format suitable for transfer and show its significant impact on system performance by comparing the transfer time of a basic SQL query with different data management systems. 
%by using a standard SQL query, sending it through an ODBC connector and then fetching the results for different data management systems. 

Back in the day popular ML tools, including RapidMiner, Weka, R's ML packages, and Python-based toolkits like SciKit-Learn and TensorFlow, only supported bulk data transfer from databases \cite{viele papperpereper}. Therefore, users had to manually load data that was already in a table format into these tools. Because of the mentioned inefficiencies this often resulted in smaller data samples being used, which is generally a bad thing in ML.
% MBY noch bisschen cleaner überleiten
%This paper examines existing serialization formats, analyzing their performance across different data sets and network scenarios. It explores the design space of serialization formats through experiments, highlighting different trade-offs in client protocol design.

This paper examines state of the art serialization formats and explores the design space of those formats in client protocol design.
Key contributions include:
\begin{itemize}
  \item Benchmarking current RSS methods, to identify data transfer inefficiencies.
  \item Investigating techniques for creating efficient serialization methods.
  \item Proposing a new column-based serialization method with significant performance improvements.
\end{itemize}

\section{State of the Art}
All remote database systems use client protocols to manage communication between server and client. This process begins with an initial authentication and an exchange of meta data. Then the client can send queries to the server. Once the server processes the query, it has to serialize the query data into a result set format, send it over the socket to the client, and then the client deserializes the data to use it.
As we pointed out, the time spent on these steps is significantly influenced by the design of the result set format. We will now explore the serialization formats used by leading systems and evaluate them.
%Protocols with heavy compression reduce data transfer time but increase serialization and deserialization time. On the other hand, simpler protocols may increase data transfer time but reduce serialization costs.

\subsection{Overview}
To evaluate the performance of various databases for large result set exports, experiments on systems like MySQL [36], PostgreSQL [32], IBM DB2 [37], "DBMS X", MonetDB [5], Hive [33], and MongoDB [23] were conducted. MySQL's client protocol with GZIP compression ("MySQL+C") was tested separately. Of course there are more database systems, but many of those adopt client protocols from more popular databases to reuse existing implementations. This selection of systems therefore seems representative for the state of the art in 2017.

The experiments focused on isolating the duration for RSS and data transfer. The TPC-H benchmark's SF10 lineitem table was loaded into each database, and data retrieval times were measured using ODBC connectors (JDBC for Hive). Netcat (nc) [12] was used as a baseline for efficient data transfer without any database overheads. The results we can see in Table 1, showed that transferring data as CSV via Netcat was drastically faster than using any tested database system. This undermines that the most time-consuming part was RSS and transfer. MongoDB had the highest overhead due to its document-based style, while MySQL+C transferred the least data due to compression.

\begin{table}[h!]
    \centering
    \caption{Time taken for result set (de)serialization + transfer when transferring the SF10 lineitem table.}
    \begin{tabular}{ p{2cm} | p{2cm}  p{2cm} }
        \textbf{System} & \textbf{Time (s)} & \textbf{Size (GB)} \\ \hline
        \textit{(Netcat)} & \textit{(10.25)} & \textit{(7.19)} \\ \hline
        MySQL & \textbf{101.22} & 7.44 \\ \hline
        DB2 & 169.48 & 7.33 \\ \hline
        DBMS X & 189.50 & 6.35 \\ \hline
        PostgreSQL & 201.89 & 10.39 \\ \hline
        MonetDB & 209.02 & 8.97 \\ \hline
        MySQL+C & 391.27 & \textbf{2.85} \\ \hline
        Hive & 627.75 & 8.69 \\ \hline
        MongoDB & 686.45 & 43.6
    \end{tabular}
\end{table}

\subsection{Network Impact}
Those experiment we talked about, were conducted with both server and client located on the same machine, so the data transfer time wasn't influenced by network factors. However, network limitations can significantly impact the performance of different client protocols. Low bandwidth makes transferring bytes more costly, favoring compression and smaller protocols, while high latency increases the cost of sending confirmation packets.
To simulate network limitations, the netem[17] utility was used to create scenarios with restricted bandwidth (0.1 ,1 , 10, 100 ms) and increased latency (10, 100, 1000 Mb/s), and 1 million rows of the lineitem table were transferred. The chosen scenarios seem representative, because they capture the typical range those parameters take in real life, from home network to internet transfers.

Higher latency adds a fixed cost to sending messages, affecting connection establishment but not large result set transfers, at least that was the assumption. Contrary to that, high latency negatively impacted all systems, due to TCP/IP layer acknowledgements, which occur frequently and slow down data transfer.
%. Contrary to that, high latency negatively impacted all systems from 0,1ms latency to 100ms latency with an factor of up to over 10x

Reduced throughput increases the cost of sending messages based on their size, penalizing protocols that send more data. With lower throughput, protocols that normally perform well, like PostgreSQL or MongoDB, suffer significantly. However, systems that use compression like MySQL+C perform better than the others as the main bottleneck shifts to data transfer, making the cost of (de)compression less significant

\subsection{Result Set Serialization}
To understand the differences in time and bytes transferred across various database protocols, their data serialization formats were examined. The authors did that by looking at the hexadecimal representations of a tiny sample table of the different protocols. This highlights the actual data versus overhead in the data representations.

\textbf{PostgreSQL}
\begin{itemize}
  \item Format: Each row is transferred in a separate message, including total length, number of fields, and field-specific data lengths.
  \item Overhead: High per-row metadata and redundant information, leading to more bytes transferred.
  \item Efficiency: Low (de)serialization costs, resulting in quick transfers if network conditions aren't limiting.
\end{itemize}

\textbf{MySQL}
\begin{itemize}
  \item Format: Uses binary encoding for metadata and text for field data. Rows begin with data length, followed by a sequence number and length-prefixed field data.
  \item Overhead: Sequence numbers are redundant due to TCP guarantees; variable-length integers for field lengths.
  \item Efficiency: Efficient binary encoding for metadata but larger data size due to text representation of field data.
\end{itemize}

\textbf{DBMS X}
\begin{itemize}
  \item Format: Terse protocol with each row prefixed by a packet header and values preceded by length in bytes.
  \item Overhead: Uses variable-length integers for lengths; employs a configurable fixed message length for batch transfers.
  \item Efficiency: Computationally intensive but allows performance optimization through configuration.
\end{itemize}

\textbf{MonetDB}
\begin{itemize}
  \item Format: Text-based serialization transferring ASCII representations of values. Rows are delimited like CSV, with additional formatting characters.
  \item Overhead: Formatting characters inflate size; conversion between binary and string formats is costly.
  \item Efficiency: Simple format but expensive in terms of computational resources due to text conversion.
\end{itemize}

\textbf{Hive}
\begin{itemize}
  \item Format: Thrift-based columnar format with serialization for structured messages, including meta data for reassembly.
  \item Overhead: Verbose serialization with significant space wasted on NULL mask encoding.
  \item Efficiency: Poor performance in benchmarks due to expensive variable-length encoding for integer columns, despite the columnar format.
\end{itemize}

%These differences explain why some systems transfer more data and take longer to serialize/deserialize result sets, as seen in the initial experiment. It can be assumed that DB2 and MongoDB weren’t mentioned here, because DB2 should look similar to the other relational DB protocols and MongoDB due to its fundamentally different approach to data storage and serialization.
%MAYBE HIER lieber den Text mit rein nehmen und representation von den dingern iwie ändern, zum beispiel format kürzen und nur kurz was zu overhead/eficiency in 1-2 stichpunten sagen
%INSBESONDERE MONGO UND DB2 MISSING REINNEHMEN EIGENTLICH

\section{Protocol Design Space}
When choosing a protocol design, there is always a core trade-off: computational versus transfer costs. For instance, when computation is negligible, using heavyweight compression techniques like XZ [31] can substantially trim transfer expenses. Conversely, if transfer costs are not a problem, opting for reduced computational overhead, even at the expense of increased data transfer, can speed up the protocol.
To benchmark all the design choices, they were isolated and tested on 3 datasets:
\begin{itemize}
  \item \textbf{lineitem} (TCP-H), resembling real-world data warehouses, features 16 columns without missing values, totaling 7.2GB.
  \item \textbf{American Community Survey (ACS)} dataset, with 274 columns and 16.1\% missing values, spans 9.1 million rows, totaling 7.0GB [6].
  \item \textbf{Airline On-Time Statistics} , with 109 columns and 55.2\% missing values, encompasses 10 million rows, totaling 3.6GB [25].
\end{itemize}
The objective was to uncover how these design choices shape serialization format performance.


\subsection{Protocol Design Choices}

\section{Implementation & Results}

\subsection{MonetDB Implementation}

\subsection{PostgreSQL Implementation}

\subsection{Evaluation}

\section{Conclusion & Future Work}

\subsection{Future Work}
Fazit nochmal labern was impact ist und wie sich anscheinend in database solutions durchsetzen aus mehreren argumenten wenn das tatsächlich der fall ist? kp
ich glaube sogar nicht so wie sich connector x anhört


% References
\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

\end{document}
